---
layout: post
title: LLMlingua
author: danghoangnhan
categories: [ LLM,LLMLingua ]
image: assets/images/llmlingua/LLMLingua_logo.png
featured: false
hidden: false
---
# LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models

### The Rise of LLMs and Their Challenges

Large language models (LLMs) have revolutionized various applications due to their remarkable capabilities. Advancements in techniques like chain-of-thought prompting and in-context learning have significantly enhanced the ability of LLMs to perform complex reasoning tasks and adapt to specific domains. However, these powerful techniques often result in increasingly long prompts, comprising tens of thousands of tokens.
This trend toward lengthy prompts presents a significant challenge. It leads to substantial computational demands and increased costs for LLM inference, hindering the broader adoption and scalability of LLMs in real-world applications. This situation has created an urgent need to balance the need for comprehensive prompts with the computational efficiency of LLMs.

![Motivation](assets/images/llmlingua/LLMLingua_motivation.png "Challenges with LLM")

In addition to the challenges posed by lengthy prompts, several inherent limitations of LLMs further underscore the need for innovative solutions:
● Token Limits: LLMs are inherently constrained by token limits, which restrict their ability to handle very long texts [Image]. This limitation can be particularly problematic when dealing with tasks that require summarizing lengthy documents or processing extensive conversational histories, as exceeding the token limit can result in information loss [Image].
● Context Loss During Fine-tuning: The process of fine-tuning LLMs for specific tasks, especially when extended over time, can lead to a phenomenon known as "catastrophic forgetting" [Image]. This occurs when the model forgets or loses context from previous instructions, negatively impacting its performance on previously learned tasks.
● High API Costs: LLMs like GPT-3.5 and GPT-4, while demonstrating impressive performance, come with substantial API costs [Image]. These costs can become prohibitive for large-scale experiments, research endeavors, or applications that require frequent interaction with the LLM.

### How LLMLingua Works

LLMLingua consists of three key modules:

* **Budget Controller**: dynamically allocates different compression ratios to various components of the prompt (instruction, demonstrations, and question). It prioritizes the instruction and the question due to their direct influence on the generated results, while applying a higher compression ratio to potentially redundant demonstrations. This module operates at the sentence or demonstration level to preserve linguistic integrity, particularly when high compression is needed.
* **Iterative Token-Level Prompt Compression (ITPC)**: This module addresses the limitations of using perplexity for prompt compression, which assumes independence between tokens. It divides the prompt into segments and iteratively compresses them, taking into account the conditional dependencies between tokens to better preserve key information.
* **Distribution Alignment**: This module tackles the discrepancy between the target LLM and the smaller language model used for prompt compression. It utilizes instruction tuning to align the distributions of the two models, using data generated by the LLM to fine-tune the smaller model.

### Key Features and Advantages

* **State-of-the-art Performance:** LLMLingua consistently outperforms existing prompt compression methods, including GPT4-Generation, Random Selection, and Selective-Context. It achieves this while enabling impressive compression ratios (up to 20x), showcasing its ability to retain critical information from the original prompt.
* **Retention of Reasoning and ICL Capabilities:** LLMLingua effectively preserves the reasoning and in-context learning capabilities of LLMs, even at high compression ratios.
* **Generalizability Across Different LLMs**: The effectiveness of LLMLingua extends beyond the GPT family of models, as demonstrated by its successful implementation with Claude-v1.3 as the target LLM.
* **Compatibility with Different Small LMs**: Although the choice of the smaller language model can impact performance, LLMLingua's design enables its adaptation to various smaller LMs, with satisfactory results achieved even with less powerful models like GPT2-Small.
* **Reduction in Generated Text Length**: Prompt compression not only saves computational resources in the input but also contributes to reduced computation in the generation stage, as evidenced by the shorter text length produced by LLMs when using compressed prompts.

### Practical Implications

* **Cost Reduction:** LLMLingua leads to significant cost savings by reducing the number of tokens processed during inference, a crucial factor considering the token-based pricing models of LLMs.
* **Enabling Longer Contexts:** By compressing prompts, LLMLingua opens up possibilities for accommodating longer contexts in LLMs, potentially enhancing their performance on tasks requiring extensive background information.
* **Potential for Downstream Task Performance Improvement:** By allowing the compression of longer prompts, LLMLingua holds the potential to enhance downstream task performance, enabling the utilization of more comprehensive prompts without incurring excessive computational costs.
* **Improved LLM Inference Efficiency:** LLMLingua's prompt compression can contribute to improved LLM inference efficiency by compressing the KV cache, further optimizing the resource utilization during model inference.

### Limitations

While LLMLingua delivers significant advantages, it's essential to acknowledge its limitations:

* **Performance Drop at Extreme Compression Ratios:** While LLMLingua mitigates performance decline at high compression ratios, exceeding certain thresholds (e.g., 25x-30x on GSM8K) can still lead to a substantial performance drop. The achievable compression ratio depends on various factors like prompt length, task type, and the number of sentences involved.
* **Tokenizer Discrepancies:** The potential for subtle differences between the tokenizers used by the smaller and target LLMs might lead to an underestimation of the prompt's token length, impacting the accuracy of compression calculations.
